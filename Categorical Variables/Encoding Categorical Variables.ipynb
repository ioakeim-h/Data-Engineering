{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "013e97e3",
   "metadata": {},
   "source": [
    "# Welcome!\n",
    "\n",
    "Machine learning models need data to be provided in numeric form. Categorical data often contain strings and should therefore be encoded. We will explore low and high cardinality encoding methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb71346",
   "metadata": {},
   "source": [
    "# <center> Low Cardinality Technique </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450eeb55",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "\n",
    "One Hot Encoding is a representation of categorical variables as binary vectors. The procedure creates a dummy variable for each unique value that exists within the categorical variable. The binary characteristic of a dummy variable is that it contains only two values: \n",
    "- 0 for data that are not represented by the dummy variable\n",
    "- 1 for data that are represented by the dummy variable\n",
    "\n",
    "One hot encoding is very popular, but it can be computationally expensive if the number of labels within a variable is high. In addition, the created binary variables predict each other perfectly, leading to multicollinearity which can be problematic for regression models. This is known as the dummy variable trap and is handled by dropping any one of them, usually the first or last. The remaining binary variables represent completely the original categorical variable. However, the drop can be problematic for tree-based algorithms because the additional label will not be considered. Tree-based algorithms are not affected by multicollinearity and the additional label should be kept - although such algorithms do not work well with high feature spaces. We may keep the extra label if recursive algorithms are going to be used.\n",
    "\n",
    "<u>Prefer to use one hot encoding when:</u>\n",
    "\n",
    "- Categorical variable is nominal\n",
    "- There are less than 10 labels within the categorical variable \n",
    "- Tree-based algorithms are not used \n",
    "- Recursive algorithms (e.g. RFE) are not used \n",
    "\n",
    "There are many libraries for the one hot encoder like *category_encoders* and *feature_engine*, but I like the sklearn version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb92f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_vars = [\"input1\", \"input2\"]\n",
    "\n",
    "ohe_enc = OneHotEncoder(sparse=False, drop = \"first\")\n",
    "enc_vars_array = ohe_enc.fit_transform(X_train[categorical_vars])\n",
    "enc_feature_names = ohe_enc.get_feature_names(categorical_vars)\n",
    "\n",
    "enc_vars_df = pd.DataFrame(enc_vars_array, columns = enc_feature_names)\n",
    "\n",
    "X_train = pd.concat([X_train.reset_index(drop=True), enc_vars_df.reset_index(drop=True)], axis = 1)\n",
    "X_train.drop(categorical_vars, axis = 1, inplace = True)\n",
    "\n",
    "X_test = ohe_enc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fface643",
   "metadata": {},
   "source": [
    "# <center> High Cardinality Techniques </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a57a8",
   "metadata": {},
   "source": [
    "## Generalized Linear Mixed Model Encoder *(supervised)*\n",
    "\n",
    "This is similar to target encoding where a category is encoded based on its average target value. The Generalized Linear Mixed Model (GLMM) version of target encoding is well supported by research and [compares well](https://www.researchgate.net/publication/350578264_Regularized_target_encoding_outperforms_traditional_methods_in_supervised_machine_learning_with_high_cardinality_features) with other encoding techniques. It may be best to use this encoder with cross validation since target encoding tends to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd10220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalized Linear Mixed Model Encoder Template\n",
    "\n",
    "from category_encoders.glmm import GLMMEncoder\n",
    "\n",
    "glmm_enc = GLMMEncoder(cols = [\"input1\", \"input2\"], \n",
    "                       handle_missing = \"return_nan\",\n",
    "                       handle_unknown = \"value\",\n",
    "                       random_state = 42,\n",
    "                       binomial_target = False) # If binomial_target = True, then target variable should be binomial.\n",
    "                                                # Elif binomial_target = False, then target must be continuous.\n",
    "                \n",
    "\n",
    "# On training data transform should be called with y, on test data without.\n",
    "X_train = glmm_enc.fit_transform(X_train, y_train)\n",
    "X_test = glmm_enc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d74bc1",
   "metadata": {},
   "source": [
    "## Weight of Evidence *(supervised)*\n",
    "\n",
    "Weight of Evidence (WoE) creates a monotonic relationship between the input and the target variable. It is defined by the logarithm of the proportion of good events divded by the proportions of bad events. \n",
    "\n",
    "- Good for regression models (including logistic regression)\n",
    "- Transformed variables are on the same scale and can be compared to explore their predictive power\n",
    "- BUT may lead to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f8d67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight of Evidence Template\n",
    "\n",
    "from category_encoders.woe import WOEEncoder\n",
    "\n",
    "woe_enc = WOEEncoder(cols = [\"input1\", \"input2\"],\n",
    "                     handle_missing = \"return_nan\",\n",
    "                     handle_unknown = \"value\",\n",
    "                     random_state = 42)\n",
    "\n",
    "# On training data transfrom should be called with y, on test data without.\n",
    "woe_enc.fit_transform(X_train, y_train)\n",
    "woe_enc.transform(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
