{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation and Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardisation rescales data to have a mean of 0 and a standard deviation of 1. Values will therefore range between -3 and 3. Unless your algorithm requires values to be positive, it is common to go with standardisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisation Template\n",
    " \n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "scale_standard = StandardScaler()\n",
    " \n",
    "# Standardize all columns\n",
    "scale_standard.fit_transform(df)\n",
    " \n",
    "# Standardize columns in list\n",
    "scale_standard.fit_transform(df[[\"col1\", \"col2\"]])\n",
    " \n",
    "# The scaler will return the scaled values in a numpy array. \n",
    "# If we want to turn them back into a pandas column we can run: \n",
    " \n",
    "df_standardised = pd.DataFrame(scale_standard.fit_transform(df),\n",
    "                               columns = df.columns)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisation - sometimes called minmax scaling- rescales data so that it exists in a range between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation Template\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    " \n",
    "scale_norm = MinMaxScaler()\n",
    "scale_norm.fit_transform(df)\n",
    "df_normalised = pd.DataFrame(scale_norm.fit_transform(df),\n",
    "                             columns = df.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling can be important but is not always a requirement. It often comes down to a trade-off between accuracy and interpretation. If you scale your values it makes it harder to understand the true meanings of any coefficients in terms of their actual values. It's often worth trying\n",
    "the model with and without scaling. If it does not appear to make any difference to accuracy then you could argue that you don't really need to do it and you will have the added bonus of easier interpretation of your model's prediction. In saying that, for algorithms such as linear regression and logistic regression scaling is often recommended but it isn't always a requirement. For decision trees and random forests it is not a requirement as they actually process all variables independently anyway. For algorithms that rely on distance-based comparisons such as KNN its always required."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
